<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[My Machine Learnings]]></title><description><![CDATA[A blog about discovering, struggling, and figuring out machine learning.]]></description><link>https://sarisboo.github.io</link><generator>GatsbyJS</generator><lastBuildDate>Tue, 15 Dec 2020 17:57:31 GMT</lastBuildDate><item><title><![CDATA[Extractive Summarizer Journey: Extracting Features]]></title><description><![CDATA[This is the second article of a series that aims to explore how to preprocess textual data and extract features to ultimately build a…]]></description><link>https://sarisboo.github.io/extractive-summarizer-journey-extracting/ext_sum_pt_2/</link><guid isPermaLink="false">https://sarisboo.github.io/extractive-summarizer-journey-extracting/ext_sum_pt_2/</guid><pubDate>Sat, 14 Nov 2020 12:09:28 GMT</pubDate><content:encoded>&lt;p&gt;This is the second article of a series that aims to explore how to preprocess textual data and extract features to ultimately build a classic extractive summarizer using a machine learning algorithm.&lt;/p&gt;
&lt;p&gt;Since we approach the extractive summarisation here as a classification problem. We need to determine which features make a sentence summary worthy. Depending on the type of text you want to summarise you might want to select particular features. Here is an example of features you can use for experimentation (This is only a preselection since we will later perform a more thorough feature selection step to determine which ones are relevant). The features can be statistical (tf-idf, length of a sentence, sentence position) or linguistic (presence of proper noun, presence of specific words, presence of verbs, pronouns, punctuation etc…). As a result, each sentence of the document will be represented by an attribute vector of features. For some features, we will make use of the &lt;code class=&quot;language-text&quot;&gt;nltk&lt;/code&gt; library.&lt;/p&gt;
&lt;h2&gt;Proper Noun Feature&lt;/h2&gt;
&lt;p&gt;The importance of a sentence can also be measured by the number of proper nouns (named entities) present. The more proper nouns, the more important the sentence and thus the more summary worthy it will be.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;# Extract Singular and plural proper nouns
def proper_noun_count_extractor(word_list):
    &amp;quot;&amp;quot;&amp;quot;
    Accepts a list of words as input and returns
    counts of NNP and NNPS words present
    &amp;quot;&amp;quot;&amp;quot;
    tagged_sentence = pos_tag(literal_eval(word_list))

    # Count Proper Nouns
    proper_noun_count = len(
        [word for word, pos in tagged_sentence if pos in [&amp;quot;NNP&amp;quot;, &amp;quot;NNPS&amp;quot;]]
    )
    return proper_noun_count&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Verb Count Feature&lt;/h2&gt;
&lt;p&gt;Verbs characterise events. The higher the verb count the more important the sentence.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;def verb_count_extractor(sentence):
    &amp;quot;&amp;quot;&amp;quot;Accepts a string as input and tokenizes and
    counts the number of verbs in the string&amp;quot;&amp;quot;&amp;quot;
    tagged_sentence = pos_tag(word_tokenize(sentence))

    # Count Verbs
    verb_count = len(
        [
            word
            for word, pos in tagged_sentence
            if pos in [&amp;quot;VB&amp;quot;, &amp;quot;VBD&amp;quot;, &amp;quot;VBG&amp;quot;, &amp;quot;VBN&amp;quot;, &amp;quot;VBP&amp;quot;, &amp;quot;VBZ&amp;quot;]
        ]
    )
    return verb_count&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Sentence Length&lt;/h2&gt;
&lt;p&gt;The intuition behind this feature is that short sentences like author names as well as unusually long, descriptive sentences are not not supposed to be part of the summary. The method employed here is a simple word count.
For this example, we will use the &lt;code class=&quot;language-text&quot;&gt;nltk&lt;/code&gt; library.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;import nltk
from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag

nltk.download(&amp;quot;averaged_perceptron_tagger&amp;quot;)

def sentence_length_extractor(sentence):
    &amp;quot;&amp;quot;&amp;quot;
    Accepts string as input, tokenizes it excluding puntuation and counts the words
    &amp;quot;&amp;quot;&amp;quot;
    # splits without punctuatiom
    tokenizer = nltk.RegexpTokenizer(r&amp;quot;\w+&amp;quot;)
    only_words_count = len(tokenizer.tokenize(sentence))

    return only_words_count&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Punctuation Feature&lt;/h2&gt;
&lt;p&gt;This is based on the intuition that sentences ending in question or exclamation marks, or sentences containing quotes are less important.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;def question_mark_finder(sentence):
    &amp;quot;&amp;quot;&amp;quot;
    Returns 1 if sentence contains question mark, 0 otherwise
    &amp;quot;&amp;quot;&amp;quot;
    if &amp;quot;?&amp;quot; in sentence:
        return 1
    else:
        return 0


def exclamation_mark_finder(sentence):
    &amp;quot;&amp;quot;&amp;quot;
    Returns 1 if sentence contains question mark, 0 otherwise
    &amp;quot;&amp;quot;&amp;quot;
    if &amp;quot;!&amp;quot; in sentence:
        return 1
    else:
        return 0


def quotes_finder(sentence):
    &amp;quot;&amp;quot;&amp;quot;
    Returns 1 if sentence contains question mark, 0 otherwise
    &amp;quot;&amp;quot;&amp;quot;
    if &amp;quot;&amp;#39;&amp;quot; or &amp;quot;`&amp;quot; in sentence:
        return 1
    else:
        return 0&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Pronoun Count Feature&lt;/h2&gt;
&lt;p&gt;Sentences with high pronoun counts are less likely to be included in summaries unless the pronouns are expanded into corresponding nouns.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;def pronoun_count(sentence):
    &amp;quot;&amp;quot;&amp;quot;Accepts string as input and counts teh number of pronouns present&amp;quot;&amp;quot;&amp;quot;

    tagged_sentence = pos_tag(word_tokenize(sentence))

    # Count Pronouns
    pron_count = len([word for word, pos in tagged_sentence if pos in [&amp;quot;PRON&amp;quot;]])
    return pron_count&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;TF-IDF&lt;/h2&gt;
&lt;p&gt;Tf-idf stands for term frequency inverse document frequency. It measures the importance of a word in a document, collection or corpus. Here we use it at the sentence level but only apply it to the training set to avoid potential data leakage. The word frequencies are subsequently stored as sparse matrices and the &lt;code class=&quot;language-text&quot;&gt;vectorizer&lt;/code&gt; is pickled for later use on the other splits for feature generation.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;import pandas as pd
import numpy as np
from ast import literal_eval
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy import sparse
import pickle


# Read the data
# Training set rediced size
train = pd.read_csv(
    &amp;quot;data/interim/cleaned/train_cleaned_step_1.csv.gz&amp;quot;,
    compression=&amp;quot;gzip&amp;quot;,
    nrows=1800000,
)
test = pd.read_csv(
    &amp;quot;data/interim/cleaned/test_cleaned_step_1.csv.gz&amp;quot;, compression=&amp;quot;gzip&amp;quot;
)
val = pd.read_csv(&amp;quot;data/interim/cleaned/val_cleaned_step_1.csv.gz&amp;quot;, compression=&amp;quot;gzip&amp;quot;)

# Drop Nas
train.dropna(inplace=True)
test.dropna(inplace=True)
val.dropna(inplace=True)

# Use training set to compute tfidf as part of feature engineering
vectorizer = TfidfVectorizer(stop_words=&amp;quot;english&amp;quot;, ngram_range=(1, 1), analyzer=&amp;quot;word&amp;quot;)
corpus_train = train[&amp;quot;sentence&amp;quot;]
corpus_test = test[&amp;quot;sentence&amp;quot;]
corpus_val = val[&amp;quot;sentence&amp;quot;]

# Fit transform train and transform test and val set using the same vectorizer
train_sparse_matrix = vectorizer.fit_transform(corpus_train)
test_sparse_matrix = vectorizer.transform(corpus_test)
val_sparse_matrix = vectorizer.transform(corpus_val)

if __name__ == &amp;quot;__main__&amp;quot;:
    # save cropped dataset
    train.to_csv(
        &amp;quot;src/features/cropped/train_cropped.csv.gz&amp;quot;,
        compression=&amp;quot;gzip&amp;quot;,
        index=False,
    )
    # Save tfidf sparse matrix for each dataset
    sparse.save_npz(
        &amp;quot;src/features/cropped/tf_idf_feature/train_sparse_matrix.npz&amp;quot;,
        train_sparse_matrix,
    )

    sparse.save_npz(
        &amp;quot;src/features/cropped/tf_idf_feature/test_sparse_matrix.npz&amp;quot;, test_sparse_matrix
    )

    sparse.save_npz(
        &amp;quot;src/features/cropped/tf_idf_feature/val_sparse_matrix.npz&amp;quot;, val_sparse_matrix
    )

    # Pickle vectorizer
    pickle.dump(
        vectorizer,
        open(&amp;quot;src/features/cropped/tf_idf_feature/tfidf_vectorizer.pickle&amp;quot;, &amp;quot;wb&amp;quot;),
    )&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Other features worth exploring&lt;/h2&gt;
&lt;p&gt;There are many feature worth exploring to improve your model.
They can be location features such as sentence position in a document or paragraph, similarity features, presence of cue-phrases (such as “in conclusion”, “this report”, “summary”…), presence of non-essential information (“because”, “additionally”, etc…) and many others.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Extractive Summarizer Journey: Preprocessing]]></title><description><![CDATA[This is the first article of a series that aims to explore how to preprocess textual data and extract features in to ultimately build a…]]></description><link>https://sarisboo.github.io/extractive-summarizer-journey-preprocessing/</link><guid isPermaLink="false">https://sarisboo.github.io/extractive-summarizer-journey-preprocessing/</guid><pubDate>Mon, 07 Sep 2020 11:00:37 GMT</pubDate><content:encoded>&lt;p&gt;This is the first article of a series that aims to explore how to preprocess textual data and extract features in to ultimately build a classic extractive summarizer using a machine learning algorithm.&lt;/p&gt;
&lt;h2&gt;Get the data&lt;/h2&gt;
&lt;p&gt;This example makes use of the &lt;code class=&quot;language-text&quot;&gt;cnn_dailymail&lt;/code&gt; dataset, it contains 2 features news &lt;code class=&quot;language-text&quot;&gt;articles&lt;/code&gt; and their corresponding &lt;code class=&quot;language-text&quot;&gt;highlights&lt;/code&gt;. Since you need to make sure you have enough instances to train, validate and test our model you can load the &lt;code class=&quot;language-text&quot;&gt;train&lt;/code&gt;, &lt;code class=&quot;language-text&quot;&gt;validation&lt;/code&gt; and &lt;code class=&quot;language-text&quot;&gt;test&lt;/code&gt; sets separately from tensorflow datasets.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;ds_train = tfds.load(name=&amp;quot;cnn_dailymail&amp;quot;, split=&amp;quot;train&amp;quot;)
ds_val = tfds.load(name=&amp;quot;cnn_dailymail&amp;quot;, split=&amp;quot;validation&amp;quot;)
ds_test = tfds.load(name=&amp;quot;cnn_dailymail&amp;quot;, split=&amp;quot;test&amp;quot;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For easier data wrangling and preprocessing, you need to transform these datasets to pandas data frames. You can either use a custom function or use the &lt;code class=&quot;language-text&quot;&gt;as_dataframe()&lt;/code&gt; method (nightly version). Either way, you might need to convert the returned data frame rows from bytes to string before applying further transformations.&lt;/p&gt;
&lt;h2&gt;Preliminary Cleanup&lt;/h2&gt;
&lt;p&gt;This can be the most time-consuming step. Once you obtain your dataset, you need to wrangle the data to see what it looks like and apply the necessary cleaning steps.
These steps can depend on the original input data. In this specific case, to extract raw text from the data, you need to remove all non-textual information by making use of regex expressions. Here is an example of text clean up for the input data using a concatenated regex expression.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;# Cleanup the datasets
# Compose concatenated regex expression to clean data faster
start_of_string = &amp;quot;^\s*&amp;quot;
remove_cnn = &amp;quot;.*\((CNN|EW.com)\)?(\s+--\s+)?&amp;quot;
remove_by_1 = (
    &amp;quot;By \.([^.*]+\.)?([^.]*\. PUBLISHED: \.[^|]*\| \.)? UPDATED:[^.]+\.[^.]+\.\s*&amp;quot;
)
remove_by_2 = &amp;quot;By \.([^.*]+\.)?(\sand[^.*]+\.\s*)?(UPDATED[^.*]+\.[^.*]+\.\s*)?(\slast[^.*]+\.\s*)?&amp;quot;
remove_last_updated = &amp;quot;Last[^.*]+\.\s&amp;quot;
remove_twitter_link = &amp;quot;By \.([^.*]+\.)\s*Follow\s@@[^.*]+\.\s+&amp;quot;
remove_published = &amp;quot;(PUBLISHED[^.*]+\.[^.*]+\.[^.*]+\.\s*)(UPDATED[^.*]+\.[^.*]+\.\s*)?&amp;quot;
# end_of_string = &amp;#39;[\&amp;#39;&amp;quot;]*\s*$&amp;#39;

r_cleanup_source = (
    start_of_string
    + &amp;quot;(&amp;quot;
    + &amp;quot;|&amp;quot;.join(
        [
            remove_cnn,
            remove_by_1,
            remove_by_2,
            remove_last_updated,
            remove_twitter_link,
            remove_published,
        ]
    )
    + &amp;quot;)&amp;quot;
)

r_cleanup = re.compile(r_cleanup_source)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You can then apply these clean up steps on the input data using the following function.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;def cleanup(text):
    return r_cleanup.sub(&amp;quot;&amp;quot;, text)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;After you get rid of all the irrelevant information for your analysis, you need to proceed to pre-processing the plain text you get to make it NLP software friendly.&lt;/p&gt;
&lt;h2&gt;Preprocessing&lt;/h2&gt;
&lt;p&gt;During the preprocessing steps, you need to regularly check for and deal with missing values before you pass them into nltk’s preprocessing steps because your code will not work when you pass in missing values.&lt;/p&gt;
&lt;h3&gt;Sentence Tokens&lt;/h3&gt;
&lt;p&gt;It is the process of breaking up text into sentences, the simplest way to do it would be to split at full stops, but there might be abbreviations like E.U, Ms. or Dr. that make this step less obvious.
Fortunately you can use libraries like nltk( &lt;code class=&quot;language-text&quot;&gt;sent_tokenize&lt;/code&gt; )or Spacy in most cases to take care of this. In the case of the &lt;code class=&quot;language-text&quot;&gt;cnn_dailymail&lt;/code&gt; the nltk sentence tokenizer inconsistently splits the sentences in cases like for example:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;sent_tokenize(&amp;#39;Bishop John Folda, of North Dakota, is taking time off after being diagnosed .He contracted the infection through contaminated food in Italy .Church members in Fargo, Grand Forks and Jamestown could have been exposed .&amp;#39;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Outputs:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;[&amp;#39;Bishop John Folda, of North Dakota, is taking time off after being diagnosed .He contracted the infection through contaminated food in Italy .Church members in Fargo, Grand Forks and Jamestown could have been exposed .&amp;#39;]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The expected output would be:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;[&amp;#39;Bishop John Folda, of North Dakota, is taking time off after being diagnosed&amp;#39;,&amp;#39;He contracted the infection through contaminated food in Italy&amp;#39;, &amp;#39;Church members in Fargo, Grand Forks and Jamestown could have been exposed&amp;#39;]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This might be because of the trailing whitespace between the last word of the sentence so for now, so you might want to use a custom sentence tokenizer to work around this limitation.&lt;/p&gt;
&lt;p&gt;One approach to this is to create a &lt;code class=&quot;language-text&quot;&gt;text_id&lt;/code&gt; out of the &lt;code class=&quot;language-text&quot;&gt;index&lt;/code&gt; column to later be able to identify which sentence belongs to each text. &lt;/p&gt;
&lt;p&gt;You then split sentences the naive way by using a simple sentence boundary regex and apply it to the data.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;def split_sentences(text):
    # Segment texts into sentences
    r_sentence_boundary = re.compile(
        r&amp;quot;\s?[.!?]\s?&amp;quot;
    )  # Modify this to not include abbreviations and other exceptions
    return r_sentence_boundary.split(text)[:-1]


# Split text by sentences
def split_by_sentence(df):
    df[&amp;quot;sentences&amp;quot;] = df[&amp;quot;article&amp;quot;].apply(lambda x: split_sentences(str(x)))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Next, you make a list of tuples to keep track of &lt;code class=&quot;language-text&quot;&gt;text_id&lt;/code&gt; by &lt;code class=&quot;language-text&quot;&gt;sentence&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;# Make a list of (text_id, sentence_list) pairs
def tup_list_maker(tup_list):
    &amp;quot;&amp;quot;&amp;quot;
    Takes a list of tuples with index 0 being the text_id and index 1 being a
    list of sentences and broadcasts the text_id to each sentence
    &amp;quot;&amp;quot;&amp;quot;
    final_list = []
    for item in tup_list:
        index = item[0]
        sentences = item[1]
        for sentence in sentences:
            pair = (index, sentence)
            final_list.append(pair)
    return final_list&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then use this function to get each sentence with its own &lt;code class=&quot;language-text&quot;&gt;text_id&lt;/code&gt;&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;def create_full_tuple(df):
    tuples = list(zip(df[&amp;quot;text_id&amp;quot;], [sentence for sentence in df[&amp;quot;sentences&amp;quot;]]))
    tup_list = tup_list_maker(tuples)
    # Converting the tuples list into a dataframe
    sentences = pd.DataFrame(tup_list, columns=[&amp;quot;text_id&amp;quot;, &amp;quot;sentence&amp;quot;])
    return sentences&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And finally you assemble the full data frame with split sentences, text ids and the &lt;code class=&quot;language-text&quot;&gt;is_summary&lt;/code&gt; columns which is the labels columns indicating which sentence belongs to the summary.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;def create_full_final_dataframe(df):

    &amp;quot;&amp;quot;&amp;quot;
    Creates the final segmented dataframe with the `is_summary` column
    &amp;quot;&amp;quot;&amp;quot;

    dataframe = make_text_id(df)
    df_article, df_highlights = split_into_2_dfs(dataframe)

    df_article[&amp;quot;sentences&amp;quot;] = df_article[&amp;quot;article&amp;quot;].apply(
        lambda x: split_sentences(str(x))
    )
    df_highlights[&amp;quot;sentences&amp;quot;] = df_highlights[&amp;quot;highlights&amp;quot;].apply(
        lambda x: split_sentences(str(x))
    )
    segmented_df_articles = create_full_tuple(df_article)
    segmented_df_highlights = create_full_tuple(df_highlights)

    # Create targets for dataframes
    segmented_df_articles[&amp;quot;is_summary_sentence&amp;quot;] = 0
    segmented_df_highlights[&amp;quot;is_summary_sentence&amp;quot;] = 1

    # Stack the 2 dataframes and order by `text_id` column
    return segmented_df_articles.append(
        segmented_df_highlights, ignore_index=True
    ).sort_values(by=[&amp;quot;text_id&amp;quot;])&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Word Tokens&lt;/h3&gt;
&lt;p&gt;For this step, it’s convenient to use nltk’s &lt;code class=&quot;language-text&quot;&gt;word_tokenize&lt;/code&gt; splitter to split each sentence into word tokens. This tokenizer splits words based on punctuation marks. Although it is not always splitting words correctly (for instance $10,000 is identified as two separate tokens [’$’, ‘10,000] and €1000 is identified as a single token [‘€1000’]),it is convenient to make use of it as a preliminary step for now.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;from nltk.tokenize import word_tokenize
def tokenizer(df, column):
    df[column].dropna(inplace=True)
    df[&amp;quot;tokens&amp;quot;] = df[column].apply(word_tokenize)
    return df&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Stopwords Removal&lt;/h3&gt;
&lt;p&gt;Next you need to remove stop words, stop words are words like as, the, of, is. They carry no real meaning for the sentence so they’re not really relevant to your analysis. The nltk library provides a list of common English stop words but it is by no means standard and stop words may vary depending on the problem you’re trying to solve.
In this case, you could use the nltk list for simplicity.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;from nltk.corpus import stopwords
from ast import literal_eval 

def stop_words_remover(tokenized_sent):
    &amp;quot;&amp;quot;&amp;quot;
    Removes stop words from a tokenized sentence
    &amp;quot;&amp;quot;&amp;quot;
    # Convert string back to list

    filtered_sentence = []
    stop_words = set(stopwords.words(&amp;quot;english&amp;quot;))
    for word in literal_eval(tokenized_sent):
        if word not in stop_words:
            filtered_sentence.append(word)
    return filtered_sentence&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Stemming (Porter Stemmer)&lt;/h3&gt;
&lt;p&gt;Stemming is the process of suffix stripping, it reduces the word to a base form that is representative of all the variants of that word.&lt;/p&gt;
&lt;p&gt;A stemmer uses a set of fixed rules to decide how the word should be stripped (it might not always end up in a linguistically correct base form).
For this analysis, you can use nltk’s Porter Stemmer for its simplicity and speed.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;from nltk.stem import PorterStemmer
from ast import literal_eval
import pandas as pd

porter = PorterStemmer()

def stemmer(stemmed_sent):
    &amp;quot;&amp;quot;&amp;quot;
    Removes stop words from a tokenized sentence
    &amp;quot;&amp;quot;&amp;quot;
    porter = PorterStemmer()
    stemmed_sentence = []
    for word in literal_eval(stemmed_sent):
        stemmed_word = porter.stem(word)
        stemmed_sentence.append(stemmed_word)
    return stemmed_sentence&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Voilà! You just cleaned and preprocessed text data for text summarization. The next step is feature engineering to extract important characteristics of the text to then feed it to our machine learning algorithm.&lt;/p&gt;</content:encoded></item></channel></rss>