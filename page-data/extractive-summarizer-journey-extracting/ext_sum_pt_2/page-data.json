{"componentChunkName":"component---src-templates-blog-post-js","path":"/extractive-summarizer-journey-extracting/ext_sum_pt_2/","result":{"data":{"site":{"siteMetadata":{"title":"My Machine Learnings"}},"markdownRemark":{"id":"2b54970e-b5e4-5949-8d62-4666866cd452","excerpt":"Since we approach the extractive summarisation here as a classification problem. We need to determine which features make a sentence summary worthy. Depending…","html":"<p>Since we approach the extractive summarisation here as a classification problem. We need to determine which features make a sentence summary worthy. Depending on the type of text you want to summarise you might want to select particular features. Here is an example of features you can use for experimentation (This is only a preselection since we will later perform a more thorough feature selection step to determine which ones are relevant). The features can be statistical (tf-idf, length of a sentence, sentence position) or linguistic (presence of proper noun, presence of specific words, presence of verbs, pronouns, punctuation etc…). As a result, each sentence of the document will be represented by an attribute vector of features. For some features, we will make use of the <code class=\"language-text\">nltk</code> library.</p>\n<h2>Proper Noun Feature</h2>\n<p>The importance of a sentence can also be measured by the number of proper nouns (named entities) present. The more proper nouns, the more important the sentence and thus the more summary worthy it will be.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\"># Extract Singular and plural proper nouns\ndef proper_noun_count_extractor(word_list):\n    &quot;&quot;&quot;\n    Accepts a list of words as input and returns\n    counts of NNP and NNPS words present\n    &quot;&quot;&quot;\n    tagged_sentence = pos_tag(literal_eval(word_list))\n\n    # Count Proper Nouns\n    proper_noun_count = len(\n        [word for word, pos in tagged_sentence if pos in [&quot;NNP&quot;, &quot;NNPS&quot;]]\n    )\n    return proper_noun_count</code></pre></div>\n<h2>Verb Count Feature</h2>\n<p>Verbs characterise events. The higher the verb count the more important the sentence.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">def verb_count_extractor(sentence):\n    &quot;&quot;&quot;Accepts a string as input and tokenizes and\n    counts the number of verbs in the string&quot;&quot;&quot;\n    tagged_sentence = pos_tag(word_tokenize(sentence))\n\n    # Count Verbs\n    verb_count = len(\n        [\n            word\n            for word, pos in tagged_sentence\n            if pos in [&quot;VB&quot;, &quot;VBD&quot;, &quot;VBG&quot;, &quot;VBN&quot;, &quot;VBP&quot;, &quot;VBZ&quot;]\n        ]\n    )\n    return verb_count</code></pre></div>\n<h2>Sentence Length</h2>\n<p>The intuition behind this feature is that short sentences like author names as well as unusually long, descriptive sentences are not not supposed to be part of the summary. The method employed here is a simple word count.\nFor this example, we will use the <code class=\"language-text\">nltk</code> library.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">import nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tag import pos_tag\n\nnltk.download(&quot;averaged_perceptron_tagger&quot;)\n\ndef sentence_length_extractor(sentence):\n    &quot;&quot;&quot;\n    Accepts string as input, tokenizes it excluding puntuation and counts the words\n    &quot;&quot;&quot;\n    # splits without punctuatiom\n    tokenizer = nltk.RegexpTokenizer(r&quot;\\w+&quot;)\n    only_words_count = len(tokenizer.tokenize(sentence))\n\n    return only_words_count</code></pre></div>\n<h2>Punctuation Feature</h2>\n<p>This is based on the intuition that sentences ending in question or exclamation marks, or sentences containing quotes are less important.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">def question_mark_finder(sentence):\n    &quot;&quot;&quot;\n    Returns 1 if sentence contains question mark, 0 otherwise\n    &quot;&quot;&quot;\n    if &quot;?&quot; in sentence:\n        return 1\n    else:\n        return 0\n\n\ndef exclamation_mark_finder(sentence):\n    &quot;&quot;&quot;\n    Returns 1 if sentence contains question mark, 0 otherwise\n    &quot;&quot;&quot;\n    if &quot;!&quot; in sentence:\n        return 1\n    else:\n        return 0\n\n\ndef quotes_finder(sentence):\n    &quot;&quot;&quot;\n    Returns 1 if sentence contains question mark, 0 otherwise\n    &quot;&quot;&quot;\n    if &quot;&#39;&quot; or &quot;`&quot; in sentence:\n        return 1\n    else:\n        return 0</code></pre></div>\n<h2>Pronoun Count Feature</h2>\n<p>Sentences with high pronoun counts are less likely to be included in summaries unless the pronouns are expanded into corresponding nouns.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">def pronoun_count(sentence):\n    &quot;&quot;&quot;Accepts string as input and counts teh number of pronouns present&quot;&quot;&quot;\n\n    tagged_sentence = pos_tag(word_tokenize(sentence))\n\n    # Count Pronouns\n    pron_count = len([word for word, pos in tagged_sentence if pos in [&quot;PRON&quot;]])\n    return pron_count</code></pre></div>\n<h2>TF-IDF</h2>\n<p>Tf-idf stands for term frequency inverse document frequency. It measures the importance of a word in a document, collection or corpus. Here we use it at the sentence level but only apply it to the training set to avoid potential data leakage. The word frequencies are subsequently stored as sparse matrices and the <code class=\"language-text\">vectorizer</code> is pickled for later use on the other splits for feature generation.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">import pandas as pd\nimport numpy as np\nfrom ast import literal_eval\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom scipy import sparse\nimport pickle\n\n\n# Read the data\n# Training set rediced size\ntrain = pd.read_csv(\n    &quot;data/interim/cleaned/train_cleaned_step_1.csv.gz&quot;,\n    compression=&quot;gzip&quot;,\n    nrows=1800000,\n)\ntest = pd.read_csv(\n    &quot;data/interim/cleaned/test_cleaned_step_1.csv.gz&quot;, compression=&quot;gzip&quot;\n)\nval = pd.read_csv(&quot;data/interim/cleaned/val_cleaned_step_1.csv.gz&quot;, compression=&quot;gzip&quot;)\n\n# Drop Nas\ntrain.dropna(inplace=True)\ntest.dropna(inplace=True)\nval.dropna(inplace=True)\n\n# Use training set to compute tfidf as part of feature engineering\nvectorizer = TfidfVectorizer(stop_words=&quot;english&quot;, ngram_range=(1, 1), analyzer=&quot;word&quot;)\ncorpus_train = train[&quot;sentence&quot;]\ncorpus_test = test[&quot;sentence&quot;]\ncorpus_val = val[&quot;sentence&quot;]\n\n# Fit transform train and transform test and val set using the same vectorizer\ntrain_sparse_matrix = vectorizer.fit_transform(corpus_train)\ntest_sparse_matrix = vectorizer.transform(corpus_test)\nval_sparse_matrix = vectorizer.transform(corpus_val)\n\nif __name__ == &quot;__main__&quot;:\n    # save cropped dataset\n    train.to_csv(\n        &quot;src/features/cropped/train_cropped.csv.gz&quot;,\n        compression=&quot;gzip&quot;,\n        index=False,\n    )\n    # Save tfidf sparse matrix for each dataset\n    sparse.save_npz(\n        &quot;src/features/cropped/tf_idf_feature/train_sparse_matrix.npz&quot;,\n        train_sparse_matrix,\n    )\n\n    sparse.save_npz(\n        &quot;src/features/cropped/tf_idf_feature/test_sparse_matrix.npz&quot;, test_sparse_matrix\n    )\n\n    sparse.save_npz(\n        &quot;src/features/cropped/tf_idf_feature/val_sparse_matrix.npz&quot;, val_sparse_matrix\n    )\n\n    # Pickle vectorizer\n    pickle.dump(\n        vectorizer,\n        open(&quot;src/features/cropped/tf_idf_feature/tfidf_vectorizer.pickle&quot;, &quot;wb&quot;),\n    )</code></pre></div>\n<h2>Other features worth exploring</h2>\n<p>There are many feature worth exploring to improve your model.\nThey can be location features such as sentence position in a document or paragraph, similarity features, presence of cue-phrases (such as “in conclusion”, “this report”, “summary”…), presence of non-essential information (“because”, “additionally”, etc…) and many others.</p>","frontmatter":{"title":"Extractive Summarizer Journey: Extracting Features","date":"November 14, 2020","description":"How to preprocess text data for NLP."}}},"pageContext":{"slug":"/extractive-summarizer-journey-extracting/ext_sum_pt_2/","previous":{"fields":{"slug":"/extractive-summarizer-journey-preprocessing/"},"frontmatter":{"title":"Extractive Summarizer Journey: Preprocessing"}},"next":null}},"staticQueryHashes":["3589320610","3966865564"]}